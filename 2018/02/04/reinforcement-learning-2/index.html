<!doctype html>
<html lang="en">
    <head>
		
        <meta http-equiv="Content-Type" content="text/html" charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content="">
        <link rel="shortcut icon" href="https://zhanggengzhi.github.io/gengzhi.github.com/assets/head.jpg"/>
        <link rel="canonical" href="http://guolinn.com/">
        <link rel="alternate" type="application/rss+xml" title="GengZhi" href="/atom.xml">
        <title>reinforcement learning(2) | GengZhi&#39;s Blog</title>
        <meta name="description" content="{{meta_description}}">

        <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="/gengzhi.github.com/styles/crisp.css">
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

    </head>
    
		<body class="post-template">
	

        <header id="header">
            <a id="logo" href="/gengzhi.github.com/"><img src="https://zhanggengzhi.github.io/gengzhi.github.com/assets/head.jpg" alt="GengZhi's Blog" /></a>
            <h1><a href="/gengzhi.github.com/">GengZhi</a></h1>
            <p></p>
            <div id="follow-icons">
                  <a href="/atom.xml"><i class="fa fa-rss-square fa-2x"></i></a>
  </div>
<h6><a href="/gengzhi.github.com/about">About</a></h6>
        </header>

        <main id="content">
        

<article class="post">
  February 4, 2018
  
    <span class="taglist">  &middot; 
    
    
      <a href='/gengzhi.github.com/tags/learning-record/'>learning record</a> 
    
    </span>
  

  <h1 class="post-title">reinforcement learning(2)</h1>
  <section class="post-content article-entry">
    <h3 id="Dynamic-programming"><a href="#Dynamic-programming" class="headerlink" title="Dynamic programming"></a>Dynamic programming</h3><p>In the last article, I introduce the basic knowledges of RL and MPDs. We can find that the problems of reinforcement learning are brought into the framework of MDPs.A Markov decision process is a 5-tuple <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;(S,A,P,R,\gamma&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;(S,A,P,R,\gamma&space;)" title="(S,A,P,R,\gamma )"></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;S" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;S" title="S"></a> is a finite set of states; <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;A" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;A" title="A"></a> is a finite set of actions; <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;P" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;P" title="P"></a> is transition probability; <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;R" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;R" title="R"></a> is the immediate reward (or expected immediate reward) received after transitioning from state s to state s’, due to action a; <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\gamma" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\gamma" title="\gamma"></a> is the discount factor, which represents the difference in importance between future rewards and present rewards. The goal of reinforcement learning is to find the best strategy <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\pi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\pi" title="\pi"></a> to maximize the expectation of the cumulative return. <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;R(\tau&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;R(\tau&space;)" title="R(\tau )"></a> is a random variable, and the random variable cannot be optimized. It can not be regarded as the objective function, and the expectation of the random variable is used as the objective function, <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\int&space;R(\tau)p_{\pi&space;}d\tau" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\int&space;R(\tau)p_{\pi&space;}d\tau" title="\int R(\tau)p_{\pi }d\tau"></a>.<br>Today, I’m going to talk about is to use the idea of dynamic programming to solve the model-based reinforcement learning problems. What are the model-based and no-model? They are distinguished by whether the transition probability is known. The transition probability is known means that the model is known. What we are going to do is to do prediction according to the model we have known. So, this is model-based. No-model is on the contrary, transition probability is unkown. Then, we also need to learn something about dynamic programming. Dynamic means the change of sequence and status, programming means optimization. Dynamic Programming is a very general solution method for problems which have two properties:1)optimal solution can be decomposed into subproblems 2)subproblems recur many times<br>Solutions can be cached and reused.<br>According to the two function we have learnde last chapter:<br><a href="https://www.codecogs.com/eqnedit.php?latex=v^{*}(s)=max_{a}R_{s}^{a}&plus;\gamma&space;\sum_{'{s}'\in&space;S}P_{s{s}'}^{a}v^{*}({s}')" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v^{*}(s)=max_{a}R_{s}^{a}&plus;\gamma&space;\sum_{'{s}'\in&space;S}P_{s{s}'}^{a}v^{*}({s}')" title="v^{*}(s)=max_{a}R_{s}^{a}+\gamma \sum_{'{s}'\in S}P_{s{s}'}^{a}v^{*}({s}')"></a><br><a href="https://www.codecogs.com/eqnedit.php?latex=q^{*}(s,a)=R_{s}^{a}&plus;\gamma&space;\sum_{'{s}'\in&space;S}P_{s{s}'}^{a}max_{'{a}'}q^{*}({s}',{a}')" target="_blank"><img src="https://latex.codecogs.com/gif.latex?q^{*}(s,a)=R_{s}^{a}&plus;\gamma&space;\sum_{'{s}'\in&space;S}P_{s{s}'}^{a}max_{'{a}'}q^{*}({s}',{a}')" title="q^{*}(s,a)=R_{s}^{a}+\gamma \sum_{'{s}'\in S}P_{s{s}'}^{a}max_{'{a}'}q^{*}({s}',{a}')"></a><br>We can know MDP problems accord with the conditions of dynamic programming from these functions. Therefore, dynamic programming can be used to solve the problems of MDP. There are three methods: value iterative, policy iterative, policy research.<br>I introduce value iterative first. Value iterative is based on <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;v_{*}(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;v_{*}(s)" title="v_{*}(s)"></a>.    </p>
<ul>
<li>At each iteration k + 1  </li>
<li>For all states <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;s\in&space;S" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;s\in&space;S" title="s\in S"></a>  </li>
<li>Update <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;v_{k&plus;1}(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;v_{k&plus;1}(s)" title="v_{k+1}(s)"></a> from <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;v_{k}({s}')" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;v_{k}({s}')" title="v_{k}({s}')"></a>  </li>
</ul>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;v_{k&plus;1}(s)=max_{a\in&space;A}(R_{s}^{a}&plus;\gamma&space;\sum_{'{s}'\in&space;S}P_{s{s}'}^{a}v_{k}({s}'))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;v_{k&plus;1}(s)=max_{a\in&space;A}(R_{s}^{a}&plus;\gamma&space;\sum_{'{s}'\in&space;S}P_{s{s}'}^{a}v_{k}({s}'))" title="v_{k+1}(s)=max_{a\in A}(R_{s}^{a}+\gamma \sum_{'{s}'\in S}P_{s{s}'}^{a}v_{k}({s}'))"></a><br>This picture can give us a more intuitive understanding:  </p>
<p><img src="./1.png" alt="Alt text"></p>
<p>But, I think the location of r in this picture is a little bit unreasonable. This picture may be more appropriate:  </p>
<p><img src="./2.png" alt="Alt text">  </p>
<p>Then, I talk about policy iterative. Policy iterative is based on <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;v_{\pi&space;}(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;v_{\pi&space;}(s)" title="v_{\pi }(s)"></a> and greedy policy improvement. <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;v_{\pi&space;}(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;v_{\pi&space;}(s)" title="v_{\pi }(s)"></a> is used for iterative policy evaluation, and then improve policy by acting greedily with respect to <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;v_{\pi&space;}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;v_{\pi&space;}" title="v_{\pi }"></a>. This is policy evaluation.  </p>
<ul>
<li>At each iteration k + 1  </li>
<li>For all states <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;s\in&space;S" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;s\in&space;S" title="s\in S"></a>  </li>
<li>Update <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;v_{k&plus;1}(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;v_{k&plus;1}(s)" title="v_{k+1}(s)"></a> from <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;v_{k}({s}')" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;v_{k}({s}')" title="v_{k}({s}')"></a>  </li>
<li>Where s’ is a successor state of s.  </li>
</ul>
<p><img src="./2.png" alt="Alt text">  </p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=v_{k&plus;1}(s)=\sum_{a\in&space;A}\pi&space;(a|s)(R_{s}^{a}&plus;\gamma&space;\sum_{'{s}'\in&space;S}P_{s{s}'}^{a}v_{k}({s}'))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_{k&plus;1}(s)=\sum_{a\in&space;A}\pi&space;(a|s)(R_{s}^{a}&plus;\gamma&space;\sum_{'{s}'\in&space;S}P_{s{s}'}^{a}v_{k}({s}'))" title="v_{k+1}(s)=\sum_{a\in A}\pi (a|s)(R_{s}^{a}+\gamma \sum_{'{s}'\in S}P_{s{s}'}^{a}v_{k}({s}'))"></a><br>Then, improve policy, <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;{\pi&space;}'=greedy(v_{\pi&space;})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;{\pi&space;}'=greedy(v_{\pi&space;})" title="{\pi }'=greedy(v_{\pi })"></a>.  </p>
<ul>
<li>Consider a deterministic policy, <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;a=\pi&space;(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;a=\pi&space;(s)" title="a=\pi (s)"></a>  </li>
<li>We can improve the policy by acting greedily, <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;{\pi&space;}'(s)=argmax_{a\in&space;A}q_{\pi&space;}(s,a)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;{\pi&space;}'(s)=argmax_{a\in&space;A}q_{\pi&space;}(s,a)" title="{\pi }'(s)=argmax_{a\in A}q_{\pi }(s,a)"></a>  </li>
<li>This improves the value from any state s over one step:<br><a href="https://www.codecogs.com/eqnedit.php?latex=q_{\pi&space;}(s,{\pi&space;}'(s))=max_{a\in&space;A}q_{\pi&space;}(s,a)\geq&space;q_{\pi&space;}(s,\pi&space;(s))=v_{\pi&space;}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?q_{\pi&space;}(s,{\pi&space;}'(s))=max_{a\in&space;A}q_{\pi&space;}(s,a)\geq&space;q_{\pi&space;}(s,\pi&space;(s))=v_{\pi&space;}" title="q_{\pi }(s,{\pi }'(s))=max_{a\in A}q_{\pi }(s,a)\geq q_{\pi }(s,\pi (s))=v_{\pi }"></a>  </li>
<li>It therefore improves the value function, <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;v_{\pi&space;}(s)\leq&space;v_{'{\pi&space;}'&space;}(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;v_{\pi&space;}(s)\leq&space;v_{'{\pi&space;}'&space;}(s)" title="v_{\pi }(s)\leq v_{'{\pi }' }(s)"></a>  </li>
<li>If improvements stop, <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;q_{\pi&space;}(s,{\pi&space;}'(s))=max_{a\in&space;A}q_{\pi&space;}(s,a)=q_{\pi&space;}(s,\pi&space;(s))=v_{\pi&space;}(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;q_{\pi&space;}(s,{\pi&space;}'(s))=max_{a\in&space;A}q_{\pi&space;}(s,a)=q_{\pi&space;}(s,\pi&space;(s))=v_{\pi&space;}(s)" title="q_{\pi }(s,{\pi }'(s))=max_{a\in A}q_{\pi }(s,a)=q_{\pi }(s,\pi (s))=v_{\pi }(s)"></a>. Then the Bellman optimality equation has been satisﬁed. <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;v_{\pi&space;}=max_{a\in&space;A}q_{\pi&space;}(s,a)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;v_{\pi&space;}=max_{a\in&space;A}q_{\pi&space;}(s,a)" title="v_{\pi }=max_{a\in A}q_{\pi }(s,a)"></a>  </li>
<li>Therefore <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;v_{\pi&space;}(s)=v_{*}(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;v_{\pi&space;}(s)=v_{*}(s)" title="v_{\pi }(s)=v_{*}(s)"></a> for all <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;s\in&space;S" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;s\in&space;S" title="s\in S"></a>, so <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\pi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\pi" title="\pi"></a> is an optimal policy    </li>
</ul>
<p>In general, after many iterations of  evaluation and improvement, this process of policy iteration always converges to <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\pi&space;_{*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\pi&space;_{*}" title="\pi _{*}"></a>.  </p>
<p><img src="./3.png" alt="Alt text">  </p>
<p>Algorithms are based on state-value function <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;v_{\pi&space;}(s,a)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;v_{\pi&space;}(s,a)" title="v_{\pi }(s,a)"></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;v_{*&space;}(s,a)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;v_{*&space;}(s,a)" title="v_{* }(s,a)"></a>, these algorithms could also apply to action-value function <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;q_{\pi&space;}(s,a)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;q_{\pi&space;}(s,a)" title="q_{\pi }(s,a)"></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;q_{*}(s,a)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;q_{*}(s,a)" title="q_{*}(s,a)"></a>.<br>DP methods described so far used synchronous backups i.e. all states are backed up in parallel. Asynchronous DP backs up states individually, in any order and for each selected state, apply the appropriate backup. Asynchronous DP also can signiﬁcantly reduce computation nd guaranteed to converge if all states continue to be selected. There are three simple ways for asynchronous dynamic programming:  </p>
<ul>
<li>In-place dynamic programming  </li>
<li>Prioritised sweeping  </li>
<li>Real-time dynamic programming  </li>
</ul>

  </section>
  <footer class="post-footer">
    <!--
    <section class="author">
      <h4>GengZhi</h4>
      <p></p>
    </section>
    -->
  </footer>
</article>

<nav class="pagination" role="pagination">
    
    <span class="page-number">•</span>
    
    <a class="older-posts" href="/gengzhi.github.com/2018/01/30/reinforcement-learning-1/">
        <!--reinforcement learning(1)--> next →
    </a>
    
</nav>


        </main>
        <footer id="footer">
            <section id="footer-message">&copy; 2018 GengZhi. All rights reserved. Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. <a href="https://github.com/guolin/crisp-hexo-theme" target="_blank">crisp</a> theme by <a href="guolin.github.io" target="_blank">Guo Lin</a>.</section>
        </footer>
    </body>
</html>


