<!doctype html>
<html lang="en">
    <head>
		
        <meta http-equiv="Content-Type" content="text/html" charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content="">
        <link rel="shortcut icon" href="https://zhanggengzhi.github.io/gengzhi.github.com/assets/head.jpg"/>
        <link rel="canonical" href="http://guolinn.com/">
        <link rel="alternate" type="application/rss+xml" title="GengZhi" href="/atom.xml">
        <title>reinforcement learning(1) | GengZhi&#39;s Blog</title>
        <meta name="description" content="{{meta_description}}">

        <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="/gengzhi.github.com/styles/crisp.css">
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

    </head>
    
		<body class="post-template">
	

        <header id="header">
            <a id="logo" href="/gengzhi.github.com/"><img src="https://zhanggengzhi.github.io/gengzhi.github.com/assets/head.jpg" alt="GengZhi's Blog" /></a>
            <h1><a href="/gengzhi.github.com/">GengZhi</a></h1>
            <p></p>
            <div id="follow-icons">
                  <a href="/atom.xml"><i class="fa fa-rss-square fa-2x"></i></a>
  </div>
<h6><a href="/gengzhi.github.com/about">About</a></h6>
        </header>

        <main id="content">
        

<article class="post">
  January 30, 2018
  
    <span class="taglist">  &middot; 
    
    
      <a href='/gengzhi.github.com/tags/learnng-recording/'>learnng recording</a> 
    
    </span>
  

  <h1 class="post-title">reinforcement learning(1)</h1>
  <section class="post-content article-entry">
    <h2 id="reinforcment-learning-intro-and-MDP"><a href="#reinforcment-learning-intro-and-MDP" class="headerlink" title="reinforcment learning intro and MDP"></a>reinforcment learning intro and MDP</h2><p>In the article, I mainly cite various material from foreign courses or on the wiki, and I will bring my own understanding. If you read my blog and want to communicate with me or find any problems, please send me email zhanggengzhi@outlook.com.:)</p>
<h3 id="reinforcemnet-learning-introduction"><a href="#reinforcemnet-learning-introduction" class="headerlink" title="reinforcemnet learning introduction"></a>reinforcemnet learning introduction</h3><p>Before introducing reinforcement learning, I show this picture first.<br><img src="./1.png" alt="Alt text"><br>From this picture, we can know the reinforcement learning is a banch of machine learning. So what are the differeces, I will give the answer at the end of this article.<br>Now, let me introduce the reiforcement learning. <strong>Reinforcement learning (RL)</strong> is an area of machine learning inspired by behaviourist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward.<br><img src="./2.png" alt="Alt text"><br>In this picture, at each step <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;t" title="t"></a>  </p>
<ul>
<li>excute action <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;A_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;A_{t}" title="A_{t}"></a>  </li>
<li>receives observation <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;O_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;O_{t}" title="O_{t}"></a>  </li>
<li>receives scalar reward <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;R_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;R_{t}" title="R_{t}"></a><br>The environment:  </li>
<li>receive action <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;A_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;A_{t}" title="A_{t}"></a>  </li>
<li>emit observation <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;O_{t&plus;1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;O_{t&plus;1}" title="O_{t+1}"></a>  </li>
<li>emit scalar reward <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;R_{t&plus;1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;R_{t&plus;1}" title="R_{t+1}"></a>  </li>
</ul>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;t" title="t"></a> increments at env. step  </p>
<p>So the history sequence of observations, actions, rewards can be represented:<br><a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;H_{t}=O_{1},R_{1},A_{1},...A_{t-1},O_{t},R_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;H_{t}=O_{1},R_{1},A_{1},...A_{t-1},O_{t},R_{t}" title="H_{t}=O_{1},R_{1},A_{1},...A_{t-1},O_{t},R_{t}"></a><br>State, which is the information used to determine what happens next, can be represented:<br><a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;S_{t}=f(H_{t})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;S_{t}=f(H_{t})" title="S_{t}=f(H_{t})"></a><br>The environment state <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;S_{t}^{e}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;S_{t}^{e}" title="S_{t}^{e}"></a> is the environment’s private representation, which the environment uses to pick the next observation/reward. The environment state is usually unvisible to the agent<br><img src="./3.png" alt="Alt text"><br>The agent state <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;S_{t}^{a}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;S_{t}^{a}" title="S_{t}^{a}"></a> is the agent’s internal representation, which the agent uses to pick the next action, and it can be used by reinforcement learning algorithms.<br><img src="./4.png" alt="Alt text"><br>The environment can be divided into fully observable environment and partically observable environment. So what differences? In the fully observable environment, <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;O_{t}=S_{t}^{e}=S_{t}^{a}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;O_{t}=S_{t}^{e}=S_{t}^{a}" title="O_{t}=S_{t}^{e}=S_{t}^{a}"></a>, means that the agent directly observes environment state, while in the partically observable environment, agent indirectly observes environment and <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;S_{t}^{e}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;S_{t}^{e}" title="S_{t}^{e}"></a> is not equal to <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;S_{t}^{a}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;S_{t}^{a}" title="S_{t}^{a}"></a>. As for a more in-depth understanding and application, I will talk in later chapters.<br>Then, I talk about the components of an RL agent. An RL agent may include one or more of these components:  </p>
<ul>
<li>policy: policy is the agen’s behavior, it’s a map from action to   m,state, and it’s always represented by <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\pi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\pi" title="\pi"></a>. By deterministic policy, <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;a=\pi&space;(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;a=\pi&space;(s)" title="a=\pi (s)"></a>. By stochastic policy, <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\pi&space;(a|s)=\mathbb{P}[A_{t}=a|S_{t}=s]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\pi&space;(a|s)=\mathbb{P}[A_{t}=a|S_{t}=s]" title="\pi (a|s)=\mathbb{P}[A_{t}=a|S_{t}=s]"></a>.     </li>
<li>value function: value function is a prediction of future reward, it’s used to evaluate the goodness/badness of states and therefore to select between actions.<br><a href="https://www.codecogs.com/eqnedit.php?latex=V_{\pi&space;}(s)=E_{\pi&space;}[R_{t&plus;1}&plus;\gamma&space;R_{t&plus;2}&plus;\gamma&space;^{2}R_{t&plus;3}&plus;...|S_{t}=s&space;]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?V_{\pi&space;}(s)=E_{\pi&space;}[R_{t&plus;1}&plus;\gamma&space;R_{t&plus;2}&plus;\gamma&space;^{2}R_{t&plus;3}&plus;...|S_{t}=s&space;]" title="V_{\pi }(s)=E_{\pi }[R_{t+1}+\gamma R_{t+2}+\gamma ^{2}R_{t+3}+...|S_{t}=s ]"></a>   </li>
<li><p>model:A model predicts what the environment will do next, P predicts the next state, R predicts the next (immediate) reward.<br><a href="https://www.codecogs.com/eqnedit.php?latex=P_{ss'}^{a}=\mathbb{P}[S_{t&plus;1}=s'|S_{t}=s,A_{t}=a]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P_{ss'}^{a}=\mathbb{P}[S_{t&plus;1}=s'|S_{t}=s,A_{t}=a]" title="P_{ss'}^{a}=\mathbb{P}[S_{t+1}=s'|S_{t}=s,A_{t}=a]"></a><br><a href="https://www.codecogs.com/eqnedit.php?latex=R_{s}^{a}=E[R_{t&plus;1}|S_{t}=s,A_{t}=a]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?R_{s}^{a}=E[R_{t&plus;1}|S_{t}=s,A_{t}=a]" title="R_{s}^{a}=E[R_{t+1}|S_{t}=s,A_{t}=a]"></a><br>These can be sum up as a picture:<br><img src="./5.png" alt="Alt text"><br>At last, I talk about RL agent categorization. According to the value function, the agent can be divided into three categories, value based, policy based, actor-critic. As shown in the table:  </p>
<p>| value based  | No Policy (Implicit), Value Function |<br>| policy based | Policy, No Value Function |<br>| actor-critic | Policy, Value Function |  </p>
</li>
</ul>
<p>According to the value function, the agent can be divided into two categories, model free and model based.  </p>
<p> | model free | Policy and/or Value Function, No Model |<br> | policy based | Policy and/or Value Function Model |  </p>
<p>I will discuss these in detail in later chapters.<br>Here, the basics of reinforcement learning have been introduced. Do you still remenber the question mentioned at beginning? yes, the differences between RL and other machine learning. We can conclude the following: </p>
<ul>
<li>There is no supervisor, only a reward signal  </li>
<li>Feedback is delayed, not instantaneous  </li>
<li>Time really matters (sequential, non i.i.d data)  </li>
<li>Agent’s actions affect the subsequent data it receives  </li>
</ul>
<h3 id="Markov-Decsion-Process-MDP"><a href="#Markov-Decsion-Process-MDP" class="headerlink" title="Markov Decsion Process(MDP)"></a>Markov Decsion Process(MDP)</h3><p>The learning process of reinforement learning is a dynamic and continuous process of interaction, and the data needed is also generated through constant interaction with the environment. Reinforcement learning is more like a human learning process. Human beings learn to walk, run, and work by interacting with their surroundings. The interaction between mankind and nature create modern civilization. In addition, deep learning such as image recognition and speech recognition deal with the problems of perception, and reinforcement learning focus on decision making. The ultimate goal of artificial intelligence is to make intelligent decisions through perception. Therefore, the deep reinforcement learning algorithm that combines the deep learning technique developed in recent years and the reinforcement learning algorithm is a promising method for human to achieve the ultimate goal of artificial intelligence. With decades of development, the researchers proposed a set of frameworks that can solve most of the reinforcement learning problems. This framework is Markov decision process, referred to as MDP.<br>Markov decision processes (MDPs) provide a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying a wide range of optimization problems solved via dynamic programming and reinforcement learning. More precisely, a Markov decision process is a discrete time stochastic control process. At each time step, the process is in some state <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;s" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;s" title="s"></a> , and the decision maker may choose any action <a href="https://www.codecogs.com/eqnedit.php?latex=a" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a" title="a"></a> that is available in state <a href="https://www.codecogs.com/eqnedit.php?latex=s" target="_blank"><img src="https://latex.codecogs.com/gif.latex?s" title="s"></a>. The process responds at the next time step by randomly moving into a new state <a href="https://www.codecogs.com/eqnedit.php?latex={s}'" target="_blank"><img src="https://latex.codecogs.com/gif.latex?{s}'" title="{s}'"></a>, and giving the decision maker a corresponding reward <a href="https://www.codecogs.com/eqnedit.php?latex=R_{a}(s,{s}')" target="_blank"><img src="https://latex.codecogs.com/gif.latex?R_{a}(s,{s}')" title="R_{a}(s,{s}')"></a>.<br>To understand MDPs, We should know what is Markov property first. The Markov property refers to the next state of the system is only related to the current state, and has nothing to do with the previous state. Mathematically, if <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;X(t),t>0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;X(t),t>0" title="X(t),t>0"></a> is a stochastic process, Markov property refers to:<br><a href="https://www.codecogs.com/eqnedit.php?latex=\mathbb{P}[X(t&plus;h)=y|X(s)=x(s),s<t]=\mathbb{P}[X(t&plus;h)=y|X(t)=x(t)],\forall&space;h>0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbb{P}[X(t&plus;h)=y|X(s)=x(s),s<t]=\mathbb{P}[X(t&plus;h)=y|X(t)=x(t)],\forall&space;h>0" title="\mathbb{P}[X(t+h)=y|X(s)=x(s),s<t]=\mathbb{P}[X(t+h)=y|X(t)=x(t)],\forall h>0"></a><br>Next, we learn Markov process. In probability theory and related fields, a Markov process is a stochastic process that satisfies the Markov property (sometimes characterized as “memorylessness”).<br>A Markov chain is a type of Markov process that has either discrete state space or discrete index set (often representing time), but the precise definition of a Markov chain varies. For example, it is common to define a Markov chain as a Markov process in either discrete or continuous time with a countable state space (thus regardless of the nature of time),but it is also common to define a Markov chain as having discrete time in either countable or continuous state space (thus regardless of the state space).<br>Markov chain is a sequence of random variables <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;X_{1},X_{2},X_{3}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;X_{1},X_{2},X_{3}" title="X_{1},X_{2},X_{3}"></a>… with the Markov property, namely that the probability of moving to the next state depends only on the present state and not on the previous states:<br><a href="https://www.codecogs.com/eqnedit.php?latex=\mathbb{P}(X_{n&plus;1}=x|X_{1}=x_{1},X_{2}=x_{2},...,X_{n}=x_{n})=\mathbb{P}(X_{n&plus;1}=x|X_{n}=x_{n})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbb{P}(X_{n&plus;1}=x|X_{1}=x_{1},X_{2}=x_{2},...,X_{n}=x_{n})=\mathbb{P}(X_{n&plus;1}=x|X_{n}=x_{n})" title="\mathbb{P}(X_{n+1}=x|X_{1}=x_{1},X_{2}=x_{2},...,X_{n}=x_{n})=\mathbb{P}(X_{n+1}=x|X_{n}=x_{n})"></a><br>Then, the third conception is Markov Decision Process. The above state sequence is called Markov chain. Given a state transition probability, there are multiple Markov chains starting from one state. For games or robots, the Markov process is not enough to describe its characteristics, because both the game and the robot interact with the environment through action and get rewards from the environment, while there are no actions and rewards in the Markov process. The Markov process that takes actions (strategies) and returns rewards is called the Markov decision process.<br>A Markov decision process is a 5-tuple <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;(S,A,P,R,\gamma&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;(S,A,P,R,\gamma&space;)" title="(S,A,P,R,\gamma )"></a>, where:  </p>
<ul>
<li><a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;S" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;S" title="S"></a> is a finite set of states  </li>
<li><a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;A" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;A" title="A"></a> is a finite set of actions  </li>
<li><a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;P_{a}(s,{s}')=\mathbb{P}(S_{t&plus;1}={s}'|s_{t}=s,a_{t}=a)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;P_{a}(s,{s}')=\mathbb{P}(S_{t&plus;1}={s}'|s_{t}=s,a_{t}=a)" title="P_{a}(s,{s}')=\mathbb{P}(S_{t+1}={s}'|s_{t}=s,a_{t}=a)"></a> is the probability that action a in state s at time t will lead to state s’ at time t+1. State transition matrix deﬁnes transition probabilities from all states s to all successor states s’.<br><a href="https://www.codecogs.com/eqnedit.php?latex=p=\begin{bmatrix}&space;p_{11}&space;&&space;...&space;&p_{1n}&space;\\&space;...&space;&&space;&...&space;\\&space;p_{n1}&space;&&space;...&space;&p_{nn}&space;\end{bmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p=\begin{bmatrix}&space;p_{11}&space;&&space;...&space;&p_{1n}&space;\\&space;...&space;&&space;&...&space;\\&space;p_{n1}&space;&&space;...&space;&p_{nn}&space;\end{bmatrix}" title="p=\begin{bmatrix} p_{11} & ... &p_{1n} \\ ... & &... \\ p_{n1} & ... &p_{nn} \end{bmatrix}"></a> where each row of the matrix sums to 1  </li>
<li><a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;R_{a}(s,{s}')" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;R_{a}(s,{s}')" title="R_{a}(s,{s}')"></a> is the immediate reward (or expected immediate reward) received after transitioning from state s to state s’, due to action a  </li>
<li><a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\gamma&space;\in&space;[0,1]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\gamma&space;\in&space;[0,1]" title="\gamma \in [0,1]"></a> is the discount factor, which represents the difference in importance between future rewards and present rewards  </li>
</ul>
<p>The core problem of MDPs is to find a “policy” for the decision maker: a function <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\pi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\pi" title="\pi"></a> that specifies the action <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\pi(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\pi(s)" title="\pi(s)"></a> that the decision maker will choose when in state s. The goal is to choose a policy <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\pi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\pi" title="\pi"></a> that will maximize some cumulative function of the random rewards, typically the expected discounted sum over a potentially infinite horizon:<br><a href="https://www.codecogs.com/eqnedit.php?latex=\sum_{t=0}^{\infty&space;}\gamma&space;^{t}R_{a_{t}'}(s_{t},s_{t&plus;1})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum_{t=0}^{\infty&space;}\gamma&space;^{t}R_{a_{t}'}(s_{t},s_{t&plus;1})" title="\sum_{t=0}^{\infty }\gamma ^{t}R_{a_{t}'}(s_{t},s_{t+1})"></a> (where we choose <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;a_{t}=\pi&space;(\pi&space;(S_{t}))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;a_{t}=\pi&space;(\pi&space;(S_{t}))" title="a_{t}=\pi (\pi (S_{t}))"></a>)<br>where <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\gamma" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\gamma" title="\gamma"></a> is the discount factor and satisfies <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;0<&space;\gamma&space;<&space;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;0<&space;\gamma&space;<&space;1" title="0< \gamma < 1"></a>. Why we need a discount number? There are some reasons.  </p>
<ul>
<li>Mathematically convenient to discount rewards  </li>
<li>Avoids inﬁnite returns in cyclic Markov processes  </li>
<li>Uncertainty about the future may not be fully represented  </li>
<li>If the reward is ﬁnancial, immediate rewards may earn more interest than delayed rewards  </li>
<li>Animal/human behaviour shows preference for immediate reward  </li>
</ul>
<p>Now, we can write value function, which gives the long-term value of state s:<br><a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;v(s)=E[G_{t}|S_{t}=s]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;v(s)=E[G_{t}|S_{t}=s]" title="v(s)=E[G_{t}|S_{t}=s]"></a>  </p>
<p>According to deduction:<br><a href="https://www.codecogs.com/eqnedit.php?latex=v(s)=E[G_{t}|S_{t}=s]&space;=E[R_{t&plus;1}&plus;\gamma&space;R_{t&plus;2}&plus;\gamma&space;^{2}R_{t&plus;3}...|S_{t}=s]&space;=E[R_{t&plus;1}&plus;\gamma&space;(R_{t&plus;2}&plus;\gamma&space;R_{t&plus;3}...)|S_{t}=s]&space;=E[R_{t&plus;1}&plus;\gamma&space;G_{t&plus;1}|S_{t}=s]&space;=E[R_{t&plus;1}&plus;\gamma&space;v(S_{t&plus;1})|S_{t}=s]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v(s)=E[G_{t}|S_{t}=s]&space;=E[R_{t&plus;1}&plus;\gamma&space;R_{t&plus;2}&plus;\gamma&space;^{2}R_{t&plus;3}...|S_{t}=s]&space;=E[R_{t&plus;1}&plus;\gamma&space;(R_{t&plus;2}&plus;\gamma&space;R_{t&plus;3}...)|S_{t}=s]&space;=E[R_{t&plus;1}&plus;\gamma&space;G_{t&plus;1}|S_{t}=s]&space;=E[R_{t&plus;1}&plus;\gamma&space;v(S_{t&plus;1})|S_{t}=s]" title="v(s)=E[G_{t}|S_{t}=s] =E[R_{t+1}+\gamma R_{t+2}+\gamma ^{2}R_{t+3}...|S_{t}=s] =E[R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+3}...)|S_{t}=s] =E[R_{t+1}+\gamma G_{t+1}|S_{t}=s] =E[R_{t+1}+\gamma v(S_{t+1})|S_{t}=s]"></a>  </p>
<p>The value function can be decomposed into two parts: immediate reward <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;R_{t&plus;1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;R_{t&plus;1}" title="R_{t+1}"></a>; discounted value of successor state <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\gamma&space;v(S_{t&plus;1})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\gamma&space;v(S_{t&plus;1})" title="\gamma v(S_{t+1})"></a>. Expressed using matrices:  </p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{bmatrix}&space;v(1)\\&space;...\\&space;v(n)&space;\end{bmatrix}=\begin{bmatrix}&space;R_{1}\\&space;...\\&space;R_{n}&space;\end{bmatrix}&plus;\gamma&space;\begin{bmatrix}&space;P_{11}&space;&&space;...&space;&P_{1n}&space;\\&space;...&space;&&space;...&space;&...&space;\\&space;P_{n1}&space;&&space;...&space;&P_{nn}&space;\end{bmatrix}\begin{bmatrix}&space;v(1)\\&space;...\\&space;v(n)&space;\end{bmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{bmatrix}&space;v(1)\\&space;...\\&space;v(n)&space;\end{bmatrix}=\begin{bmatrix}&space;R_{1}\\&space;...\\&space;R_{n}&space;\end{bmatrix}&plus;\gamma&space;\begin{bmatrix}&space;P_{11}&space;&&space;...&space;&P_{1n}&space;\\&space;...&space;&&space;...&space;&...&space;\\&space;P_{n1}&space;&&space;...&space;&P_{nn}&space;\end{bmatrix}\begin{bmatrix}&space;v(1)\\&space;...\\&space;v(n)&space;\end{bmatrix}" title="\begin{bmatrix} v(1)\\ ...\\ v(n) \end{bmatrix}=\begin{bmatrix} R_{1}\\ ...\\ R_{n} \end{bmatrix}+\gamma \begin{bmatrix} P_{11} & ... &P_{1n} \\ ... & ... &... \\ P_{n1} & ... &P_{nn} \end{bmatrix}\begin{bmatrix} v(1)\\ ...\\ v(n) \end{bmatrix}"></a><br><img src="./9.png" alt="Alt text"><br>As the picture shows, the value function can also be writen as <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;P_{s,{s}'}^{\pi&space;}=\sum_{a\in&space;A}\pi&space;(a|s)P_{s,{s}'&space;}^{a}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;P_{s,{s}'}^{\pi&space;}=\sum_{a\in&space;A}\pi&space;(a|s)P_{s,{s}'&space;}^{a}" title="P_{s,{s}'}^{\pi }=\sum_{a\in A}\pi (a|s)P_{s,{s}' }^{a}"></a>, <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;R_{s}^{\pi&space;}=\sum_{a\in&space;A}\pi&space;(a|s)R_{s&space;}^{a}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;R_{s}^{\pi&space;}=\sum_{a\in&space;A}\pi&space;(a|s)R_{s&space;}^{a}" title="R_{s}^{\pi }=\sum_{a\in A}\pi (a|s)R_{s }^{a}"></a><br>In the same way, we can get state-action function <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;q_{\pi&space;}(s,a)=E_{\pi&space;}[R_{t&plus;1}&plus;\gamma&space;q(S_{t&plus;1},A_{t&plus;1})|S_{t}=s,A_{t}=a]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;q_{\pi&space;}(s,a)=E_{\pi&space;}[R_{t&plus;1}&plus;\gamma&space;q(S_{t&plus;1},A_{t&plus;1})|S_{t}=s,A_{t}=a]" title="q_{\pi }(s,a)=E_{\pi }[R_{t+1}+\gamma q(S_{t+1},A_{t+1})|S_{t}=s,A_{t}=a]"></a>, what’s the relationship between value function and value-action function? This picture can give us answer:<br><img src="./10.png" alt="Alt text"><br>Apparently, we can get these function:<br><a href="https://www.codecogs.com/eqnedit.php?latex=v_{\pi&space;}(s)=\sum_{\alpha&space;\in&space;A}\pi&space;(a|s)q_{\pi&space;}(s,a)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_{\pi&space;}(s)=\sum_{\alpha&space;\in&space;A}\pi&space;(a|s)q_{\pi&space;}(s,a)" title="v_{\pi }(s)=\sum_{\alpha \in A}\pi (a|s)q_{\pi }(s,a)"></a> <a href="https://www.codecogs.com/eqnedit.php?latex==\sum_{\alpha&space;\in&space;A}\pi&space;(a|s)(R_{s}^{a}&plus;\gamma&space;\sum_{'{s}'\in&space;S&space;}P_{s{s}'}^{a}v_{\pi&space;}({s}'))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?=\sum_{\alpha&space;\in&space;A}\pi&space;(a|s)(R_{s}^{a}&plus;\gamma&space;\sum_{'{s}'\in&space;S&space;}P_{s{s}'}^{a}v_{\pi&space;}({s}'))" title="=\sum_{\alpha \in A}\pi (a|s)(R_{s}^{a}+\gamma \sum_{'{s}'\in S }P_{s{s}'}^{a}v_{\pi }({s}'))"></a><br><a href="https://www.codecogs.com/eqnedit.php?latex=q_{\pi&space;}(s,a)=R_{s}^{a}&plus;\gamma&space;\sum_{'{s}'\in&space;S}P_{s{s}'}^{a}v_{\pi&space;}({s}')=R_{s}^{a}&plus;\gamma&space;\sum_{'{s}'\in&space;S}P_{s{s}'}^{a}\sum_{'{a}'\in&space;A}\pi&space;({a}'|{s}')q_{\pi&space;}({s}',{a}')" target="_blank"><img src="https://latex.codecogs.com/gif.latex?q_{\pi&space;}(s,a)=R_{s}^{a}&plus;\gamma&space;\sum_{'{s}'\in&space;S}P_{s{s}'}^{a}v_{\pi&space;}({s}')=R_{s}^{a}&plus;\gamma&space;\sum_{'{s}'\in&space;S}P_{s{s}'}^{a}\sum_{'{a}'\in&space;A}\pi&space;({a}'|{s}')q_{\pi&space;}({s}',{a}')" title="q_{\pi }(s,a)=R_{s}^{a}+\gamma \sum_{'{s}'\in S}P_{s{s}'}^{a}v_{\pi }({s}')=R_{s}^{a}+\gamma \sum_{'{s}'\in S}P_{s{s}'}^{a}\sum_{'{a}'\in A}\pi ({a}'|{s}')q_{\pi }({s}',{a}')"></a>   </p>

  </section>
  <footer class="post-footer">
    <!--
    <section class="author">
      <h4>GengZhi</h4>
      <p></p>
    </section>
    -->
  </footer>
</article>

<nav class="pagination" role="pagination">
    
    <a class="newer-posts" href="/gengzhi.github.com/2018/02/04/reinforcement-learning-2/">
        ← prev <!--reinforcement learning(2)-->
    </a>
    
    <span class="page-number">•</span>
    
    <a class="older-posts" href="/gengzhi.github.com/2018/01/29/Welcome-to-my-blog/">
        <!--Welcome to my blog--> next →
    </a>
    
</nav>


        </main>
        <footer id="footer">
            <section id="footer-message">&copy; 2018 GengZhi. All rights reserved. Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. <a href="https://github.com/guolin/crisp-hexo-theme" target="_blank">crisp</a> theme by <a href="guolin.github.io" target="_blank">Guo Lin</a>.</section>
        </footer>
    </body>
</html>


