<!doctype html>
<html lang="en">
    <head>
		
        <meta http-equiv="Content-Type" content="text/html" charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content="">
        <link rel="shortcut icon" href="https://zhanggengzhi.github.io/gengzhi.github.com/assets/head.jpg"/>
        <link rel="canonical" href="http://guolinn.com/">
        <link rel="alternate" type="application/rss+xml" title="GengZhi" href="/atom.xml">
        <title>reinforcement learning(1) | GengZhi&#39;s Blog</title>
        <meta name="description" content="{{meta_description}}">

        <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="/gengzhi.github.com/styles/crisp.css">
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

    </head>
    
		<body class="post-template">
	

        <header id="header">
            <a id="logo" href="/gengzhi.github.com/"><img src="https://zhanggengzhi.github.io/gengzhi.github.com/assets/head.jpg" alt="GengZhi's Blog" /></a>
            <h1><a href="/gengzhi.github.com/">GengZhi</a></h1>
            <p></p>
            <div id="follow-icons">
                  <a href="/atom.xml"><i class="fa fa-rss-square fa-2x"></i></a>
  </div>
<h6><a href="/gengzhi.github.com/about">About</a></h6>
        </header>

        <main id="content">
        

<article class="post">
  January 30, 2018
  
    <span class="taglist">  &middot; 
    
    
      <a href='/gengzhi.github.com/tags/learnng-recording/'>learnng recording</a> 
    
    </span>
  

  <h1 class="post-title">reinforcement learning(1)</h1>
  <section class="post-content article-entry">
    <h1 id="reinforcment-learning-intro-and-MDP"><a href="#reinforcment-learning-intro-and-MDP" class="headerlink" title="reinforcment learning intro and MDP"></a>reinforcment learning intro and MDP</h1><p>In the article, I mainly cite various material from foreign courses or on the wiki, and I will bring my own understanding. If you read my blog and want to communicate with me or find any problems, please send me email zhanggengzhi@outlook.com.:)</p>
<h2 id="reinforcemnet-learning-introduction"><a href="#reinforcemnet-learning-introduction" class="headerlink" title="reinforcemnet learning introduction"></a>reinforcemnet learning introduction</h2><p>Before introducing reinforcement learning, I show this picture first.<br><img src="./reinforcement-learning-1/1.png" alt="Alt text"><br>From this picture, we can know the reinforcement learning is a banch of machine learning. So what are the differeces, I will give the answer at the end of this article.<br>Now, let me introduce the reiforcement learning. <strong>Reinforcement learning (RL)</strong> is an area of machine learning inspired by behaviourist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward.<br><img src="./reinforcement-learning-1/2.png" alt="Alt text"><br>In this picture, at each step <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;t" title="t"></a>   </p>
<ul>
<li>excute action <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;A_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;A_{t}" title="A_{t}"></a>  </li>
<li>receives observation <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;O_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;O_{t}" title="O_{t}"></a>  </li>
<li>receives scalar reward <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;R_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;R_{t}" title="R_{t}"></a><br>The environment:  </li>
<li>receive action <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;A_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;A_{t}" title="A_{t}"></a>  </li>
<li>emit observation <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;O_{t&plus;1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;O_{t&plus;1}" title="O_{t+1}"></a>  </li>
<li>emit scalar reward <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;R_{t&plus;1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;R_{t&plus;1}" title="R_{t+1}"></a><br><a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;t" title="t"></a> increments at env. step<br>So the history sequence of observations, actions, rewards can be represented:<br><a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;H_{t}=O_{1},R_{1},A_{1},...A_{t-1},O_{t},R_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;H_{t}=O_{1},R_{1},A_{1},...A_{t-1},O_{t},R_{t}" title="H_{t}=O_{1},R_{1},A_{1},...A_{t-1},O_{t},R_{t}"></a><br>State, which is the information used to determine what happens next, can be represented:<br><a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;S_{t}=f(H_{t})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;S_{t}=f(H_{t})" title="S_{t}=f(H_{t})"></a><br>The environment state <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;S_{t}^{e}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;S_{t}^{e}" title="S_{t}^{e}"></a> is the environment’s private representation, which the environment uses to pick the next observation/reward. The environment state is usually unvisible to the agent<br><img src="./reinforcement-learning-1/3.png" alt="Alt text"><br>The agent state <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;S_{t}^{a}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;S_{t}^{a}" title="S_{t}^{a}"></a> is the agent’s internal representation, which the agent uses to pick the next action, and it can be used by reinforcement learning algorithms.<br><img src="./reinforcement-learning-1/4.png" alt="Alt text"><br>The environment can be divided into fully observable environment and partically observable environment. So what differences? In the fully observable environment, <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;O_{t}=S_{t}^{e}=S_{t}^{a}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;O_{t}=S_{t}^{e}=S_{t}^{a}" title="O_{t}=S_{t}^{e}=S_{t}^{a}"></a>, means that the agent directly observes environment state, while in the partically observable environment, agent indirectly observes environment and <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;S_{t}^{e}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;S_{t}^{e}" title="S_{t}^{e}"></a> is not equal to <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;S_{t}^{a}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;S_{t}^{a}" title="S_{t}^{a}"></a>. As for a more in-depth understanding and application, I will talk in later chapters.<br>Then, I talk about the components of an RL agent. An RL agent may include one or more of these components:  </li>
<li>policy: policy is the agen’s behavior, it’s a map from action to   m,state, and it’s always represented by <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\pi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\pi" title="\pi"></a>. By deterministic policy, <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;a=\pi&space;(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;a=\pi&space;(s)" title="a=\pi (s)"></a>. By stochastic policy, <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\pi&space;(a|s)=\mathbb{P}[A_{t}=a|S_{t}=s]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\pi&space;(a|s)=\mathbb{P}[A_{t}=a|S_{t}=s]" title="\pi (a|s)=\mathbb{P}[A_{t}=a|S_{t}=s]"></a>.     </li>
<li>value function: value function is a prediction of future reward, it’s used to evaluate the goodness/badness of states and therefore to select between actions.<br><a href="https://www.codecogs.com/eqnedit.php?latex=V_{\pi&space;}(s)=E_{\pi&space;}[R_{t&plus;1}&plus;\gamma&space;R_{t&plus;2}&plus;\gamma&space;^{2}R_{t&plus;3}&plus;...|S_{t}=s&space;]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?V_{\pi&space;}(s)=E_{\pi&space;}[R_{t&plus;1}&plus;\gamma&space;R_{t&plus;2}&plus;\gamma&space;^{2}R_{t&plus;3}&plus;...|S_{t}=s&space;]" title="V_{\pi }(s)=E_{\pi }[R_{t+1}+\gamma R_{t+2}+\gamma ^{2}R_{t+3}+...|S_{t}=s ]"></a>   </li>
<li>model:A model predicts what the environment will do next, P predicts the next state, R predicts the next (immediate) reward.<br><a href="https://www.codecogs.com/eqnedit.php?latex=P_{ss'}^{a}=\mathbb{P}[S_{t&plus;1}=s'|S_{t}=s,A_{t}=a]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P_{ss'}^{a}=\mathbb{P}[S_{t&plus;1}=s'|S_{t}=s,A_{t}=a]" title="P_{ss'}^{a}=\mathbb{P}[S_{t+1}=s'|S_{t}=s,A_{t}=a]"></a><br><a href="https://www.codecogs.com/eqnedit.php?latex=R_{s}^{a}=E[R_{t&plus;1}|S_{t}=s,A_{t}=a]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?R_{s}^{a}=E[R_{t&plus;1}|S_{t}=s,A_{t}=a]" title="R_{s}^{a}=E[R_{t+1}|S_{t}=s,A_{t}=a]"></a><br>These can be sum up as a picture:<br><img src="./reinforcement-learning-1/5.png" alt="Alt text"><br>At last, I talk about RL agent categorization. According to the value function, the agent can be divided into three categories, value based, policy based, actor-critic. As shown in the table:<br>| value based | No Policy (Implicit), Value Function |<br>| policy based | Policy, No Value Function |<br>| actor-critic | Policy, Value Function |<br>According to the value function, the agent can be divided into two categories, model free and model based.<br>| model free | Policy and/or Value Function, No Model |<br>| policy based | Policy and/or Value Function Model |<br>I will discuss these in detail in later chapters.<br>Here, the basics of reinforcement learning have been introduced. Do you still remenber the question mentioned at beginning? yes, the differences between RL and other machine learning. We can conclude the following:  </li>
<li>There is no supervisor, only a reward signal  </li>
<li>Feedback is delayed, not instantaneous  </li>
<li>Time really matters (sequential, non i.i.d data)  </li>
<li>Agent’s actions affect the subsequent data it receives </li>
</ul>

  </section>
  <footer class="post-footer">
    <!--
    <section class="author">
      <h4>GengZhi</h4>
      <p></p>
    </section>
    -->
  </footer>
</article>

<nav class="pagination" role="pagination">
    
    <span class="page-number">•</span>
    
    <a class="older-posts" href="/gengzhi.github.com/2018/01/29/Welcome-to-my-blog/">
        <!--Welcome to my blog--> next →
    </a>
    
</nav>


        </main>
        <footer id="footer">
            <section id="footer-message">&copy; 2018 GengZhi. All rights reserved. Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. <a href="https://github.com/guolin/crisp-hexo-theme" target="_blank">crisp</a> theme by <a href="guolin.github.io" target="_blank">Guo Lin</a>.</section>
        </footer>
    </body>
</html>


